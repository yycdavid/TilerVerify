from .utils import (ensure_dir, get_nr_correct, default_dataset_root,
                    sqr_hinge_loss, torch_as_npy)
from .attack_impl import pgd_batch, loss_fn_map as pgd_loss_fn_map
from .mp_verifier_attack import VerifierAdvBatchFinder
from . import net_bin

import torch
import torch.utils
import torch.nn as nn
import torch.optim as optim
import torchvision.utils as vutils
import numpy as np

import argparse
import time
import sys
import gc
import threading
from pathlib import Path
from tqdm import tqdm

train_loss_fn_map = {
    'xent': nn.CrossEntropyLoss,
    'sqr-hinge': lambda: sqr_hinge_loss,
}

logmsg = print

class ModelChooser:
    _args = None
    _device = None
    _model = None
    _out_root = None

    _data = None
    _best_acc = 0

    def __init__(self, args, device, model, trainloader):
        self._args = args
        self._device = device
        self._model = model
        self._out_root = Path(args.output)
        self._data = [i for _, i in zip(range(args.choose_nr_minibatch),
                                        trainloader)]

    def __call__(self, epoch):
        if self._args.checkpoint and epoch % self._args.checkpoint == 0:
            self._model.save_to_file(self._out_root / f'chkpoint-{epoch}.pth')

        if epoch < self._args.epoch - self._args.choose_epoch:
            return

        model = self._model
        model.eval()

        nr_correct = 0
        tot = 0
        for inputs, labels in self._data:
            inputs = inputs.to(self._device)
            labels = labels.to(self._device)
            outputs = model(inputs)
            nr_correct += get_nr_correct(outputs, labels)
            tot += inputs.size(0)

        acc = nr_correct / tot
        logmsg(f'model choosing acc @{epoch}: {acc*100:.2f}%')
        if acc > self._best_acc:
            logmsg(f'best model update at @{epoch}')
            self._best_acc = acc
            model.save_to_file(self._out_root / f'last.pth')


class WeightClip:
    _weights = None
    def __init__(self, model):
        weights = []
        for i in model.features:
            if isinstance(i, (net_bin.BinConv2d, net_bin.BinLinear)):
                weights.append(i.weight)
                if (mask := i._parameters.get('weight_mask')) is not None:
                    weights.append(mask)
        logmsg(f'WeightClip: {len(weights)} weight params')
        self._weights = weights

    def __call__(self):
        with torch.no_grad():
            for i in self._weights:
                i.clamp_(-1, 1)

class EpochHyperParam:
    __slots__ = [
        # epoch number
        'epoch',
        # learning rate
        'lr',
        # adversarial training perturbation bound / iteration steps
        'adv_epsilon', 'adv_steps',
        # activation stabilization coefficient (at most one enabled)
        'act_stabilize_abs', 'act_stabilize_smp',
        # ratio of adv images that should be generated by a verifier
        'verifier_mix',
    ]

    @classmethod
    def from_epoch(cls, args, epoch):
        s = cls()
        s.epoch = epoch
        s.lr = args.lr * (0.5 ** (epoch // 150))
        p = {'epoch': epoch}
        s.adv_epsilon = float(eval(args.adv_schedule, p))
        s.adv_steps = int(eval(args.adv_steps, p))
        s.act_stabilize_abs = float(eval(args.act_abs_stabilize, p))
        s.act_stabilize_smp = float(eval(args.act_smp_stabilize, p))
        s.verifier_mix = float(eval(args.verifier_mix, p))
        assert (s.adv_epsilon >= 0 and s.adv_steps >= 0 and
                s.act_stabilize_abs >= 0 and s.act_stabilize_smp >= 0 and
                0 <= s.verifier_mix <= 1)
        assert not s.act_stabilize_abs or not s.act_stabilize_smp
        return s


class MinibatchState:
    __slots__ = [
        # total loss
        'loss',
        # classification loss
        'clsfy_loss',
        # string describing adversarial training
        'adv_str',
        # a function to get activation stabilization loss description
        'asloss_str_fn',
        # a function to get bias regularizer loss (a.k.a. CBD loss)
        'bloss_str_fn',
    ]

    def __init__(self):
        self.loss = 0
        self.adv_str = ''
        self.asloss_str_fn = lambda: ''
        self.bloss_str_fn = lambda: ''

    def setup_asloss(self, asloss: torch.Tensor, scale: torch.Tensor):
        asloss_s = scale * asloss
        self.loss = self.loss + asloss_s
        self.asloss_str_fn = lambda: (
            f'asloss={asloss_s.item():.2f}/{asloss.item():.2f} ')


class TrainingLoop:
    _args = None
    _device = None
    _net = None

    _verifier_adv_finder = None

    _out_root = None

    _trainloader = None
    _testloader = None

    _model_chooser = None

    _criterion = None
    _optimizer = None
    _weight_clip = None
    _bias_regularizer = None

    _avg_test_acc = None

    _prev_end_time = None

    def __init__(self, args, device, net,
                 verifier_adv_finder: VerifierAdvBatchFinder):
        self._args = args
        self._device = device
        self._net = net
        self._verifier_adv_finder = verifier_adv_finder

        self._out_root = Path(args.output)

        self._trainloader = net.make_dataset_loader(args, True)
        self._testloader = net.make_dataset_loader(args, False)

        self._model_chooser = ModelChooser(
            args, device, net, self._trainloader)

        self._criterion = train_loss_fn_map[args.loss]()
        self._optimizer = optim.Adam(net.parameters(), lr=args.lr)
        self._weight_clip = WeightClip(net)
        self._bias_regularizer = net_bin.BiasRegularizer(
            args.bias_hinge_coeff, args.bias_hinge_thresh, net)

        self._avg_test_acc = []
        self._prev_end_time = time.time()

    def train_epoch(self, epoch):
        self._net.train()
        ep = EpochHyperParam.from_epoch(self._args, epoch)

        for param_group in self._optimizer.param_groups:
            param_group['lr'] = ep.lr

        logmsg(f'epoch: {epoch} {ep.lr=} {ep.adv_epsilon=} {ep.adv_steps=}')

        for idx, (inputs, labels) in enumerate(self._trainloader):
            self._train_minibatch(
                idx, inputs.to(self._device), labels.to(self._device), ep)

        if (spar_stat := self._net.get_sparsity_stat()) is not None:
            spar_parts, spar_nz, spar_tot = spar_stat
            spar_parts = ', '.join('{:.2f}%'.format(i*100) for i in spar_parts)
            logmsg(f'sparsity@{epoch}: {spar_parts} ; '
                   f'{spar_nz}/{spar_tot}={spar_nz/spar_tot*100:.2f}%')

        test_acc, test_loss = self._eval_model_on_test()
        logmsg(f'test@{epoch}: acc={test_acc*100:.2f}% '
               f'avg_acc={np.mean(self._avg_test_acc)*100:.2f}% '
               f'loss={test_loss:.2f}')

        self._model_chooser(epoch)

    def _eval_model_on_test(self):
        """:return: test accuracy, test loss"""
        nr_test_correct = 0
        nr_test_tot = 0
        test_loss_sum = 0
        self._net.eval()
        for inputs, labels in self._testloader:
            inputs = inputs.to(self._device)
            labels = labels.to(self._device)
            outputs = self._net(inputs)
            loss = self._criterion(outputs, labels)
            size = int(inputs.size(0))
            test_loss_sum += float(loss.item()) * size
            nr_test_correct += get_nr_correct(outputs, labels)
            nr_test_tot += size

        test_acc = nr_test_correct / nr_test_tot
        avg = self._avg_test_acc
        if len(avg) == 10:
            del avg[0]
        avg.append(test_acc)

        return test_acc, test_loss_sum / nr_test_tot

    def _train_minibatch(self, idx, inputs, labels, ep: EpochHyperParam):
        ms = MinibatchState()
        inputs_adv = self._get_training_adv(inputs, labels, ep, ms)

        if not idx:
            vutils.save_image(inputs,
                              str(self._out_root / 'training_sample.png'),
                              normalize=False)
            if inputs_adv is not inputs:
                vutils.save_image(
                    inputs_adv,
                    str(self._out_root / 'training_sample_adv.png'),
                    normalize=False)

        time_data = time.time() - self._prev_end_time

        self._optimizer.zero_grad()
        outputs = self._compute_losses(inputs, inputs_adv, labels, ep, ms)
        ms.loss.backward()
        self._optimizer.step()
        self._weight_clip()

        time_train = time.time() - self._prev_end_time

        nr_correct_cache = None
        def nr_correct():
            nonlocal nr_correct_cache
            if nr_correct_cache is None:
                nr_correct_cache = get_nr_correct(outputs, labels)
            return nr_correct_cache

        if idx % 20 == 0:
            acc = nr_correct() / inputs.size(0)
            if ms.adv_str:
                ms.adv_str += f':{inputs.size(0)-nr_correct()} '
            logmsg(
                f'[{ep.epoch}, {idx}] '
                f'loss={ms.clsfy_loss.item():.2f}/{ms.loss.item():.2f} '
                f'{ms.bloss_str_fn()}{ms.asloss_str_fn()}{ms.adv_str}'
                f'acc={acc*100:.2f}% '
                f'tdata/train=({time_data:.3f},{time_train:.3f})')

        self._prev_end_time = time.time()

    def _compute_losses(self, inputs, inputs_adv, labels, ep, ms):
        with self._bias_regularizer:
            if ep.act_stabilize_smp and ep.adv_epsilon:
                outputs, asloss = self._net.forward_with_multi_sample(
                    inputs, inputs_adv, ep.adv_epsilon)
                ms.setup_asloss(asloss, ep.act_stabilize_smp)
            else:
                outputs = self._net(inputs_adv)

        ms.clsfy_loss = self._criterion(outputs, labels)
        ms.loss += ms.clsfy_loss

        if self._bias_regularizer.coeff:
            ms.loss += self._bias_regularizer.loss
            ms.bloss_str_fn = lambda: (
                f'bloss={self._bias_regularizer.loss.item():.2f}/'
                f'{self._bias_regularizer.loss_max.item():.2f} ')

        if ep.act_stabilize_abs and ep.adv_epsilon:
            ms.setup_asloss(
                self._net.compute_act_stabilizing_loss_abstract(
                    inputs, ep.adv_epsilon),
                ep.act_stabilize_abs
            )

        return outputs

    def _get_training_adv(self, inputs, labels,
                          ep: EpochHyperParam, ms: MinibatchState):
        if ep.adv_epsilon <= 1e-9:
            return inputs

        def run_pgd(x0, adv_init):
            if not ep.adv_steps:
                if adv_init is not None:
                    return adv_init
                return x0
            ret, nr_correct = pgd_batch(
                self._net, x0, pgd_loss_fn_map[self._args.adv_loss](labels),
                ep.adv_epsilon, ep.adv_steps, restore=True,
                adv_init=adv_init,
            )
            if not ms.adv_str:
                ms.adv_str = f'pgd={x0.size(0)-nr_correct}'
            return ret

        inputs_adv = run_pgd(inputs, None)
        if ep.verifier_mix > 0:
            nr_adv_goal = max(
                int(round(ep.verifier_mix * inputs.shape[0])), 1)
            inputs_adv, nr_adv_verifier, nr_adv_ref = (
                self._verifier_adv_finder.find_adv_batch(
                    self._net, inputs, inputs_adv, labels, ep.adv_epsilon,
                    nr_adv_goal))
            ms.adv_str = (
                f'vrf=({nr_adv_verifier}/{nr_adv_goal}+{nr_adv_ref})')

            # optimize the adv starting from verifier results
            inputs_adv = run_pgd(inputs, inputs_adv)

        return inputs_adv


def main():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('net', help='name of network')
    parser.add_argument('output')
    parser.add_argument('--load', help='load saved network')
    parser.add_argument('--start-epoch', type=int, default=0,
                        help='starting epoch number, for correctly '
                        'continuing training')
    parser.add_argument('--epoch', type=int, default=200)
    parser.add_argument('--lr', type=float, default=1e-4,
                        help='learning rate')
    parser.add_argument('--batchsize', type=int, default=128)
    parser.add_argument('--workers', type=int, default=2,
                        help='number of CPU workers for data loading')
    parser.add_argument(
        '--data', default=default_dataset_root(),
        help='dir for training data')
    parser.add_argument(
        '--bias-hinge-thresh', type=float, default=0,
        help='threshold for hinge loss of bias'
    )
    parser.add_argument(
        '--bias-hinge-coeff', type=float, default=0,
        help='coefficient for hinge loss of bias'
    )
    parser.add_argument('--input-quantization', type=float, default=1/255,
                        help='input quantization step on 0-1 scale')
    parser.add_argument('--adv-steps', default='0',
                        help='a python expression for PGD steps for '
                        'adversarial training')
    parser.add_argument('--adv-schedule', default='0',
                        help='a python expression to set adv epsilon for each '
                        'epoch')
    parser.add_argument('--adv-loss', default='xent',
                        choices=pgd_loss_fn_map.keys(),
                        help='loss function for adversarial example mining')
    parser.add_argument('--act-abs-stabilize', default='0',
                        help='a python expression for action stablization '
                        'coeff by abstract interpretation')
    parser.add_argument('--act-smp-stabilize', default='0',
                        help='a python expression for action stablization '
                        'coeff by multiple sampling')
    parser.add_argument('--loss', choices=train_loss_fn_map.keys(),
                        default='xent',
                        help='classification loss function for training')
    parser.add_argument('--set-global-param', default=[], action='append',
                        help='set global parameter in the net_bin module')
    parser.add_argument('--choose-nr-minibatch', type=int, default=40,
                        help='number of minibatches from training data to '
                        'choose best model')
    parser.add_argument('--choose-epoch', type=int, default=5,
                        help='last epochs to enter best model choice')
    parser.add_argument('--verifier-nproc', type=int, default=3,
                        help='number of processes to run the verifier')
    parser.add_argument(
        '--verifier-mix', default='0',
        help='a python expression for max number of adversarial examples from '
        'verifier; use 0 to disable verifier. It is a value between [0, 1] '
        'relative to the batch size')
    parser.add_argument('--checkpoint', type=int, default=0,
                        help='epochs to save checkpoint')
    args = parser.parse_args()

    for i in args.set_global_param:
        k, v = i.split('=')
        assert k.startswith('g_') and hasattr(net_bin, k), (
            f'no such global param {k}')
        setattr(net_bin, k, eval(v, {'net_bin': net_bin}))

    ensure_dir(args.output)

    net_class = getattr(net_bin, args.net)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    net = net_class.create_with_load(
        fpath=args.load,
        kwargs={'quant_step': args.input_quantization},
        enforce_state_load=True).to(device)

    out_root = Path(args.output)
    with (out_root / 'log.txt').open('w') as fout:
        pass
    def logmsg_(msg):
        print(msg)
        with (out_root / 'log.txt').open('a') as fout:
            print(msg, file=fout)
    global logmsg
    logmsg = logmsg_

    logmsg(f'argv: {sys.argv}')
    logmsg(f'parsed args: {args}')
    logmsg('global params: {}'.format(
        [(k, v) for k, v in net_bin.__dict__.items()
         if k.startswith('g_')]))
    logmsg(f'network: {net}')

    verifier_adv_finder = VerifierAdvBatchFinder(args.verifier_nproc)
    trainer = TrainingLoop(args, device, net, verifier_adv_finder)
    try:
        for i in range(args.start_epoch, args.epoch):
            trainer.train_epoch(i)
    finally:
        verifier_adv_finder.close()

    with (out_root / 'finish_mark').open('w') as fout:
        pass

if __name__ == '__main__':
    main()
